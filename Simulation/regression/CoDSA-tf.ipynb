{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d629d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toy_reg import *\n",
    "import ot\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f862d998",
   "metadata": {},
   "source": [
    "## pretrained AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c95044",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre, y_train_pre, regions_train = generate_imbalanced_data(5000,5000, seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5913f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_tensor =  combine_XY(X_train_pre, y_train_pre)\n",
    "\n",
    "set_seed(111)\n",
    "# Create a DataLoader\n",
    "# Initialize and train the autoencoder\n",
    "autoencoder = Autoencoder(input_dim=6, latent_dim=3, hidden_dim=512)\n",
    "autoencoder = train_autoencoder(autoencoder, XY_tensor, num_epochs=1000, learning_rate=1e-4, device='cuda')\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26135e",
   "metadata": {},
   "source": [
    "## CoDSA with pretrained AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9b4ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_result = []\n",
    "list_origin = []\n",
    "W_list = [[] for j in range(10)]\n",
    "ce_tf=[]\n",
    "\n",
    "\n",
    "num_sim=10\n",
    "num_split= len(ratio_list )\n",
    "best_indices = np.empty((num_sim, num_split, 2), dtype=int)\n",
    "# best_test_errors[i,j] will store the corresponding test error.\n",
    "best_test_errors = np.empty((num_sim, num_split))\n",
    "\n",
    "\n",
    "\n",
    "# Set data sizes.\n",
    "n_minority = 800\n",
    "n_majority = 3200\n",
    "n_val = 400\n",
    "n_test = 800\n",
    "n_train =4000\n",
    "m=2*max(candidate_m_syn)\n",
    "\n",
    "#hyperparameter sets\n",
    "ratio_list = [x/10 for x in list(range(1,11))] #split ratio r\n",
    "candidate_m_syn=  [int(n_train*round(x * 0.1, 2)) for x in range(21)] #synthetic sample size m/n\n",
    "candidate_alpha_scale= [0.1, 0.2, 0.3,0.4, 0.5,0.6, 0.7,0.8, 0.9] #alpha\n",
    "\n",
    "X_total, y_total, regions_total = generate_imbalanced_data(\n",
    "        n_minority + n_val + n_test, \n",
    "        n_majority + n_val + n_test, \n",
    "        seed=0\n",
    "    )\n",
    "\n",
    "if not os.path.exists(\"synthetic_tf\"):\n",
    "    os.makedirs(\"synthetic_tf\")\n",
    "\n",
    "for j in range(10):\n",
    "    set_seed(j)\n",
    "    \n",
    "    # Separate the minority and majority groups based on region labels.\n",
    "    # Here, we assume that region label 0 indicates minority and 1 indicates majority.\n",
    "    minority_idx = np.where(regions_total == 0)[0]\n",
    "    majority_idx = np.where(regions_total == 1)[0]\n",
    "\n",
    "    # Shuffle indices separately\n",
    "    np.random.shuffle(minority_idx)\n",
    "    np.random.shuffle(majority_idx)\n",
    "\n",
    "    # For minority:\n",
    "    min_train_idx = minority_idx[:n_minority]\n",
    "    min_val_idx   = minority_idx[n_minority : n_minority + n_val]\n",
    "    min_test_idx  = minority_idx[n_minority + n_val : n_minority + n_val + n_test]\n",
    "\n",
    "    # For majority:\n",
    "    maj_train_idx = majority_idx[:n_majority]\n",
    "    maj_val_idx   = majority_idx[n_majority : n_majority + n_val]\n",
    "    maj_test_idx  = majority_idx[n_majority + n_val : n_majority + n_val + n_test]\n",
    "\n",
    "    # Combine directly for each set\n",
    "    X_train_orig = np.vstack((X_total[min_train_idx], X_total[maj_train_idx]))\n",
    "    y_train_orig = np.vstack((y_total[min_train_idx], y_total[maj_train_idx]))\n",
    "    regions_train = np.concatenate((regions_total[min_train_idx], regions_total[maj_train_idx]))\n",
    "\n",
    "    X_val = np.vstack((X_total[min_val_idx], X_total[maj_val_idx]))\n",
    "    y_val = np.vstack((y_total[min_val_idx], y_total[maj_val_idx]))\n",
    "    regions_val = np.concatenate((regions_total[min_val_idx], regions_total[maj_val_idx]))\n",
    "\n",
    "    X_test = np.vstack((X_total[min_test_idx], X_total[maj_test_idx]))\n",
    "    y_test = np.vstack((y_total[min_test_idx], y_total[maj_test_idx]))\n",
    "    regions_test = np.concatenate((regions_total[min_test_idx], regions_total[maj_test_idx]))\n",
    "\n",
    "\n",
    "    XY_train_orig = combine_XY(X_train_orig, y_train_orig)\n",
    "    \n",
    "    n_train= XY_train_orig.shape[0]\n",
    "\n",
    "    origin_model, mse_val_origin, mse_test_origin, y_val_opred, y_test_opred = train_and_evaluate(\n",
    "    X_train_orig, y_train_orig, X_val, y_val, X_test, y_test)\n",
    "    list_origin.append([mse_val_origin, mse_test_origin])\n",
    "    print([mse_val_origin, mse_test_origin])\n",
    "\n",
    "    all_result=[]\n",
    "    for split_ratio in ratio_list:\n",
    "\n",
    "        n_diff1 = int(split_ratio * n_minority)\n",
    "        n_diff2 = int(split_ratio * n_majority)\n",
    "\n",
    "        # Shuffle indices for each group.\n",
    "        indices_min = np.arange(n_minority)\n",
    "        np.random.shuffle(indices_min)\n",
    "        indices_maj = np.arange(n_majority)\n",
    "        np.random.shuffle(indices_maj)\n",
    "\n",
    "        # Extract diffusion subset using the shuffled indices.\n",
    "        XY_diff = np.vstack((\n",
    "            XY_train_orig[indices_min[:n_diff1], :],\n",
    "            XY_train_orig[n_minority + indices_maj[:n_diff2], :]\n",
    "        ))\n",
    "        regions_diff = np.concatenate((\n",
    "            regions_train[indices_min[:n_diff1]],\n",
    "            regions_train[n_minority + indices_maj[:n_diff2]]\n",
    "        ))\n",
    "        \n",
    "        if split_ratio==1:\n",
    "            XY_reg=None\n",
    "        else:\n",
    "            XY_reg = np.vstack((XY_train_orig[n_diff1:n_minority,:],XY_train_orig[(n_minority+n_diff2):,:]))\n",
    "        \n",
    "\n",
    "        result=[]\n",
    "        file_path = f\"synthetic_tf/synthetic_X_seed{j}_ratio{split_ratio:.1f}.npy\"\n",
    "        if os.path.exists(file_path):          \n",
    "\n",
    "            # Build file paths.\n",
    "            file_X = f\"synthetic_tf/synthetic_X_seed{j}_ratio{split_ratio:.1f}.npy\"\n",
    "            file_y = f\"synthetic_tf/synthetic_y_seed{j}_ratio{split_ratio:.1f}.npy\"\n",
    "\n",
    "            # Load the synthetic data.\n",
    "            X_syn_full = np.load(file_X)\n",
    "            y_syn_full = np.load(file_y)\n",
    "            \n",
    "        else:\n",
    "            with torch.no_grad():        \n",
    "                U_diff=autoencoder.encoder(torch.tensor(XY_diff, dtype=torch.float32).to('cuda'))\n",
    "\n",
    "            model = ConditionalDiffusionModel(text_dim=3, cond_dim=1, hidden_dim= 1024, time_embed_dim=128,\n",
    "                                              num_fc_blocks=10,dropout = 1e-5)\n",
    "            sampler = DDIMSampler(device=device,noise_steps=1000)\n",
    "            model = model.to(device)\n",
    "            trained_model = train_conditional_diffusion(model, U_diff, regions_diff, sampler, num_epochs=2000, batch_size=64, seed=j,device='cuda')\n",
    "\n",
    "\n",
    "            X_syn_full, y_syn_full = generate(trained_model,autoencoder.decoder,m,0.5,sampler,seed =j)        \n",
    "            \n",
    "            np.save(f\"synthetic_tf/synthetic_X_seed{j}_ratio{split_ratio:.1f}.npy\", X_syn_full)\n",
    "            np.save(f\"synthetic_tf/synthetic_y_seed{j}_ratio{split_ratio:.1f}.npy\", y_syn_full)\n",
    "        \n",
    "\n",
    "        X_truth,y_truth,_=generate_imbalanced_data(int(m/2), int(m/2),seed=j)\n",
    "        truth_samples=combine_XY(X_truth,y_truth)\n",
    "        gen_samples = combine_XY(X_syn_full, y_syn_full)      \n",
    "        # Compute cost matrix (Euclidean distances)\n",
    "        M = ot.dist(truth_samples, gen_samples, metric='euclidean')\n",
    "        a = np.ones((m,)) / m\n",
    "        b = np.ones((m,)) / m\n",
    "        W_distance = ot.emd2(a, b, M)\n",
    "        W_list[j].append(W_distance)\n",
    "        print(\"Wasserstein distance:\", W_distance)\n",
    "        \n",
    "        \n",
    "        for m_syn in candidate_m_syn:   \n",
    "            # Generate synthetic data for minority (region 1)\n",
    "           \n",
    "            tmp=[]\n",
    "            if m_syn==0 and split_ratio ==1:\n",
    "                for alpha_scale in candidate_alpha_scale:\n",
    "                    tmp.append([np.inf, np.inf])\n",
    "                result.append(tmp)\n",
    "                continue\n",
    "            for alpha_scale in candidate_alpha_scale:\n",
    "                # Adjust synthetic data: if alpha_scale < 1, use a fraction; if > 1, replicate data accordingly.\n",
    "                if m_syn ==0:\n",
    "                    X_train_combined = XY_reg[:,:-1]\n",
    "                    y_train_combined = XY_reg[:,-1:]    \n",
    "                else:\n",
    "\n",
    "                #X_train_combined,y_train_combined = generate(trained_model,m_syn,alpha_scale)\n",
    "                    X_train_combined = np.vstack((X_syn_full[:int(m_syn*alpha_scale),], X_syn_full[int(m/2):int(m/2+m_syn*(1-alpha_scale)),]))\n",
    "                    y_train_combined = np.vstack((y_syn_full[:int(m_syn*alpha_scale),], y_syn_full[int(m/2):int(m/2+m_syn*(1-alpha_scale)),]))\n",
    "\n",
    "                    if split_ratio <1:\n",
    "                        X_train_combined = np.vstack((X_train_combined,XY_reg[:,:-1]))\n",
    "                        y_train_combined = np.vstack((y_train_combined,XY_reg[:,-1:]))\n",
    "\n",
    "\n",
    "                # Train and evaluate on validation set\n",
    "                _, mse_val, mse_test, _, _ = train_and_evaluate(\n",
    "                    X_train_combined, y_train_combined, X_val, y_val, X_test, y_test)\n",
    "\n",
    "                print(f\"m_syn={m_syn}, alpha_scale={alpha_scale} -> Validation MSE: {mse_val:.4f} -> Test MSE: {mse_test:.4f}\")\n",
    "\n",
    "                tmp.append([mse_val, mse_test])\n",
    "            result.append(tmp)\n",
    "\n",
    "        all_result.append(result)\n",
    "    \n",
    "    ax= [all_result[x][:21] for x in range(10)] \n",
    "    ax=np.array(ax)\n",
    "    num_sim, num_split, num_m, num_min_ratio = ax.shape\n",
    "    val_errors = ax[: , :, :, 0]\n",
    "\n",
    "    # Find the indices (o, k, l) that minimize the validation error.\n",
    "    o, k, l = np.unravel_index(np.argmin(val_errors), val_errors.shape)\n",
    "    best_ratio = ratio_list[o]\n",
    "    best_m = candidate_m_syn[k]\n",
    "    best_alpha = candidate_alpha_scale[l]\n",
    "\n",
    "    \n",
    "    n_diff1 = int(best_ratio * n_minority)\n",
    "    n_diff2 = int(best_ratio * n_majority)\n",
    "\n",
    "    # Shuffle indices for each group.\n",
    "    indices_min = np.arange(n_minority)\n",
    "    np.random.shuffle(indices_min)\n",
    "    indices_maj = np.arange(n_majority)\n",
    "    np.random.shuffle(indices_maj)\n",
    "\n",
    "    # Extract diffusion subset using the shuffled indices.\n",
    "    XY_diff = np.vstack((\n",
    "        XY_train_orig[indices_min[:n_diff1], :],\n",
    "        XY_train_orig[n_minority + indices_maj[:n_diff2], :]\n",
    "    ))\n",
    "    regions_diff = np.concatenate((\n",
    "        regions_train[indices_min[:n_diff1]],\n",
    "        regions_train[n_minority + indices_maj[:n_diff2]]\n",
    "    ))\n",
    "\n",
    "    XY_reg = np.vstack((XY_train_orig[n_diff1:n_minority,:],XY_train_orig[(n_minority+n_diff2):,:]))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Build file paths.\n",
    "    file_X = f\"synthetic_tf/synthetic_X_seed{j}_ratio{best_ratio:.1f}.npy\"\n",
    "    file_y = f\"synthetic_tf/synthetic_y_seed{j}_ratio{best_ratio:.1f}.npy\"\n",
    "\n",
    "    # Load the synthetic data.\n",
    "    X_syn_full = np.load(file_X)\n",
    "    y_syn_full = np.load(file_y)\n",
    "\n",
    "    X_train_combined = np.vstack((\n",
    "        X_syn_full[:int(best_m*best_alpha), :],\n",
    "        X_syn_full[int(m/2):int(m/2+best_m*(1-best_alpha)), :]\n",
    "    ))\n",
    "    y_train_combined = np.vstack((\n",
    "        y_syn_full[:int(best_m*best_alpha), :],\n",
    "        y_syn_full[int(m/2):int(m/2+best_m*(1-best_alpha)), :]\n",
    "    ))\n",
    "\n",
    "    X_train_combined = np.vstack((X_train_combined, XY_reg[:, :-1]))\n",
    "    y_train_combined = np.vstack((y_train_combined, XY_reg[:, -1:]))\n",
    "\n",
    "    ce_one={}\n",
    "    # Train and evaluate on validation set\n",
    "    _, mse_val, mse_test, ypred_val, ypred_test = train_and_evaluate(\n",
    "        X_train_combined, y_train_combined, X_val, y_val, X_test, y_test)\n",
    "    ce_one['avg']=mse_test\n",
    "    \n",
    "    _, mse_val, mse_test, ypred_val, ypred_test = train_and_evaluate(\n",
    "    X_train_combined, y_train_combined, X_val, y_val, X_test[regions_test==1,], y_test[regions_test==1,])\n",
    "    ce_one['major']=mse_test\n",
    "        \n",
    "        \n",
    "    _, mse_val, mse_test, ypred_val, ypred_test = train_and_evaluate(\n",
    "    X_train_combined, y_train_combined, X_val, y_val, X_test[regions_test==0,], y_test[regions_test==0,])\n",
    "    ce_one['minor']=mse_test       \n",
    "    \n",
    "    ce_tf.append(ce_one)\n",
    "    list_result.append(all_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8a8b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"result/res_tf.pkl\", \"wb\") as f:\n",
    "    pickle.dump([list_result,W_list,list_origin,ce_tf], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ee4c344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "with open(\"result/res_tf.pkl\", \"rb\") as f:\n",
    "    list_result,W_list,list_origin,ce_tf,_ = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7858cb8c",
   "metadata": {},
   "source": [
    "### Summary statistics for Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a5da117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42010826531690004, 0.762041202574482, 0.591074733945691]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean([x['major'] for x in ce_tf]),np.mean([x['minor'] for x in ce_tf]),np.mean([x['avg'] for x in ce_tf])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d7560f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.104574241487262, 0.501055635078655, 0.27182784429640644]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.std([x['major'] for x in ce_tf]),np.std([x['minor'] for x in ce_tf]),np.std([x['avg'] for x in ce_tf])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0051b364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.3",
   "language": "python",
   "name": "python3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
