{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "569d59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toy_reg import *\n",
    "import ot\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ea295",
   "metadata": {},
   "source": [
    "## CoDSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877cdce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate imbalanced data\n",
    "n_minority= 800\n",
    "n_majority = 3200\n",
    "n_val=400\n",
    "n_test=800\n",
    "n_train=4000\n",
    "\n",
    "ratio_list = [x/10 for x in list(range(1,11))]\n",
    "candidate_m_syn=  [int(n_train*round(x * 0.1, 2)) for x in range(21)]\n",
    "candidate_alpha_scale= [0.1, 0.2, 0.3,0.4, 0.5,0.6, 0.7,0.8, 0.9]\n",
    "m=2*max(candidate_m_syn)\n",
    "\n",
    "num_sim=10\n",
    "num_split= len(ratio_list )\n",
    "W_list=[[] for j in range(num_sim)]\n",
    "X_total, y_total, regions_total = generate_imbalanced_data(\n",
    "        n_minority + n_val + n_test, \n",
    "        n_majority + n_val + n_test, \n",
    "        seed=0\n",
    "    )\n",
    "\n",
    "list_result=[]\n",
    "ce_nf=[]\n",
    "list_origin=[]    \n",
    "\n",
    "if not os.path.exists(\"synthetic_nf\"):\n",
    "    os.makedirs(\"synthetic_nf\")\n",
    "    \n",
    "    \n",
    "    \n",
    "for j in range(num_sim):\n",
    "    set_seed(j)\n",
    "    \n",
    "    minority_idx = np.where(regions_total == 0)[0]\n",
    "    majority_idx = np.where(regions_total == 1)[0]\n",
    "\n",
    "    # Shuffle indices separately\n",
    "    np.random.shuffle(minority_idx)\n",
    "    np.random.shuffle(majority_idx)\n",
    "\n",
    "    # For minority:\n",
    "    min_train_idx = minority_idx[:n_minority]\n",
    "    min_val_idx   = minority_idx[n_minority : n_minority + n_val]\n",
    "    min_test_idx  = minority_idx[n_minority + n_val : n_minority + n_val + n_test]\n",
    "\n",
    "    # For majority:\n",
    "    maj_train_idx = majority_idx[:n_majority]\n",
    "    maj_val_idx   = majority_idx[n_majority : n_majority + n_val]\n",
    "    maj_test_idx  = majority_idx[n_majority + n_val : n_majority + n_val + n_test]\n",
    "\n",
    "    # Combine directly for each set\n",
    "    X_train_orig = np.vstack((X_total[min_train_idx], X_total[maj_train_idx]))\n",
    "    y_train_orig = np.vstack((y_total[min_train_idx], y_total[maj_train_idx]))\n",
    "    regions_train = np.concatenate((regions_total[min_train_idx], regions_total[maj_train_idx]))\n",
    "\n",
    "    X_val = np.vstack((X_total[min_val_idx], X_total[maj_val_idx]))\n",
    "    y_val = np.vstack((y_total[min_val_idx], y_total[maj_val_idx]))\n",
    "    regions_val = np.concatenate((regions_total[min_val_idx], regions_total[maj_val_idx]))\n",
    "\n",
    "    X_test = np.vstack((X_total[min_test_idx], X_total[maj_test_idx]))\n",
    "    y_test = np.vstack((y_total[min_test_idx], y_total[maj_test_idx]))\n",
    "    regions_test = np.concatenate((regions_total[min_test_idx], regions_total[maj_test_idx]))\n",
    "\n",
    "    # Combine training X and y for diffusion training\n",
    "    XY_train_orig = combine_XY(X_train_orig, y_train_orig)\n",
    "    n_train= XY_train_orig.shape[0]\n",
    "\n",
    "    origin_model, mse_val_origin, mse_test_origin, y_val_opred, y_test_opred = train_and_evaluate(\n",
    "    X_train_orig, y_train_orig, X_val, y_val, X_test, y_test)\n",
    "    list_origin.append([mse_val_origin, mse_test_origin])\n",
    "    print([mse_val_origin, mse_test_origin])\n",
    "\n",
    "    all_result=[]\n",
    "    for split_ratio in ratio_list:\n",
    "        n_diff1 = int(split_ratio * n_minority)\n",
    "        n_diff2 = int(split_ratio * n_majority)\n",
    "\n",
    "        # Shuffle indices for each group.\n",
    "        indices_min = np.arange(n_minority)\n",
    "        np.random.shuffle(indices_min)\n",
    "        indices_maj = np.arange(n_majority)\n",
    "        np.random.shuffle(indices_maj)\n",
    "\n",
    "        # Extract diffusion subset using the shuffled indices.\n",
    "        XY_diff = np.vstack((\n",
    "            XY_train_orig[indices_min[:n_diff1], :],\n",
    "            XY_train_orig[n_minority + indices_maj[:n_diff2], :]\n",
    "        ))\n",
    "        regions_diff = np.concatenate((\n",
    "            regions_train[indices_min[:n_diff1]],\n",
    "            regions_train[n_minority + indices_maj[:n_diff2]]\n",
    "        ))\n",
    "        if split_ratio==1:\n",
    "            XY_reg=None\n",
    "        else:\n",
    "            XY_reg = np.vstack((XY_train_orig[n_diff1:n_minority,:],XY_train_orig[(n_minority+n_diff2):,:]))\n",
    "\n",
    "\n",
    "        result=[]\n",
    "        \n",
    "        file_path = f\"synthetic_nf/synthetic_X_seed{j}_ratio{split_ratio:.1f}.npy\"\n",
    "        if os.path.exists(file_path):          \n",
    "\n",
    "            # Build file paths.\n",
    "            file_X = f\"synthetic_nf/synthetic_X_seed{j}_ratio{split_ratio:.1f}.npy\"\n",
    "            file_y = f\"synthetic_nf/synthetic_y_seed{j}_ratio{split_ratio:.1f}.npy\"\n",
    "\n",
    "            # Load the synthetic data.\n",
    "            X_syn_full = np.load(file_X)\n",
    "            y_syn_full = np.load(file_y)\n",
    "            \n",
    "        else:\n",
    "            model = ConditionalDiffusionModel(text_dim=6, cond_dim=1, hidden_dim= 1024, time_embed_dim=128,\n",
    "                                          num_fc_blocks=10,dropout=1e-5)\n",
    "            sampler = DDIMSampler(device=device,noise_steps=1000)\n",
    "            model = model.to(device)\n",
    "            trained_model = train_conditional_diffusion(model, XY_diff, regions_diff, sampler, num_epochs=2000, batch_size=128, seed=j,device='cuda')\n",
    "\n",
    "            \n",
    "            X_syn_full, y_syn_full = generate(trained_model,None, m,0.5,sampler,seed =j)\n",
    "        \n",
    "            np.save(f\"synthetic_nf/synthetic_X_seed{j}_ratio{split_ratio:.1f}.npy\", X_syn_full)\n",
    "            np.save(f\"synthetic_nf/synthetic_y_seed{j}_ratio{split_ratio:.1f}.npy\", y_syn_full)\n",
    "        \n",
    "\n",
    "        X_truth,y_truth,_=generate_imbalanced_data(int(m/2), int(m/2),seed=j)\n",
    "        truth_samples=combine_XY(X_truth,y_truth)\n",
    "        gen_samples = combine_XY(X_syn_full, y_syn_full)      # replace with your generated samples\n",
    "        # Compute cost matrix (Euclidean distances)\n",
    "        M = ot.dist(truth_samples, gen_samples, metric='euclidean')\n",
    "        a = np.ones((m,)) / m\n",
    "        b = np.ones((m,)) / m\n",
    "        W_distance = ot.emd2(a, b, M)\n",
    "        W_list[j].append(W_distance)\n",
    "        print(\"Wasserstein distance:\", W_distance)\n",
    "        \n",
    "        \n",
    "        for m_syn in candidate_m_syn:   \n",
    "            # Generate synthetic data for minority (region 1)\n",
    "           \n",
    "            tmp=[]\n",
    "            if m_syn==0 and split_ratio ==1:\n",
    "                for alpha_scale in candidate_alpha_scale:\n",
    "                    tmp.append([np.inf, np.inf])\n",
    "                result.append(tmp)\n",
    "                continue\n",
    "            \n",
    "            for alpha_scale in candidate_alpha_scale:\n",
    "                # Adjust synthetic data: if alpha_scale < 1, use a fraction; if > 1, replicate data accordingly.\n",
    "                if m_syn ==0:\n",
    "                    X_train_combined = XY_reg[:,:-1]\n",
    "                    y_train_combined = XY_reg[:,-1:]    \n",
    "                else:\n",
    "\n",
    "                #X_train_combined,y_train_combined = generate(trained_model,m_syn,alpha_scale)\n",
    "                    X_train_combined = np.vstack((X_syn_full[:int(m_syn*alpha_scale),], X_syn_full[int(m/2):int(m/2+m_syn*(1-alpha_scale)),]))\n",
    "                    y_train_combined = np.vstack((y_syn_full[:int(m_syn*alpha_scale),], y_syn_full[int(m/2):int(m/2+m_syn*(1-alpha_scale)),]))\n",
    "\n",
    "                    if split_ratio <1:\n",
    "                        X_train_combined = np.vstack((X_train_combined,XY_reg[:,:-1]))\n",
    "                        y_train_combined = np.vstack((y_train_combined,XY_reg[:,-1:]))\n",
    "                \n",
    "\n",
    "                # Train and evaluate on validation set\n",
    "                _, mse_val, mse_test, _, _ = train_and_evaluate(\n",
    "                    X_train_combined, y_train_combined, X_val, y_val, X_test, y_test)\n",
    "\n",
    "                print(f\"m_syn={m_syn}, alpha_scale={alpha_scale} -> Validation MSE: {mse_val:.4f} -> Test MSE: {mse_test:.4f}\")\n",
    "\n",
    "                tmp.append([mse_val, mse_test])\n",
    "            result.append(tmp)\n",
    "\n",
    "\n",
    "        all_result.append(result)\n",
    "        \n",
    "        \n",
    "    ax= [all_result[x][:21] for x in range(10)] \n",
    "    ax=np.array(ax)\n",
    "    num_sim, num_split, num_m, num_min_ratio = ax.shape\n",
    "    val_errors = ax[: , :, :, 0]\n",
    "\n",
    "    # Find the indices (k, l) that minimize the validation error.\n",
    "    o, k, l = np.unravel_index(np.argmin(val_errors), val_errors.shape)\n",
    "\n",
    "    best_ratio = ratio_list[o]\n",
    "    best_m = candidate_m_syn[k]\n",
    "    best_alpha = candidate_alpha_scale[l]\n",
    "\n",
    "    \n",
    "    n_diff1 = int(best_ratio * n_minority)\n",
    "    n_diff2 = int(best_ratio * n_majority)\n",
    "\n",
    "    # Shuffle indices for each group.\n",
    "    indices_min = np.arange(n_minority)\n",
    "    np.random.shuffle(indices_min)\n",
    "    indices_maj = np.arange(n_majority)\n",
    "    np.random.shuffle(indices_maj)\n",
    "\n",
    "    # Extract diffusion subset using the shuffled indices.\n",
    "    XY_diff = np.vstack((\n",
    "        XY_train_orig[indices_min[:n_diff1], :],\n",
    "        XY_train_orig[n_minority + indices_maj[:n_diff2], :]\n",
    "    ))\n",
    "    regions_diff = np.concatenate((\n",
    "        regions_train[indices_min[:n_diff1]],\n",
    "        regions_train[n_minority + indices_maj[:n_diff2]]\n",
    "    ))\n",
    "\n",
    "    XY_reg = np.vstack((XY_train_orig[n_diff1:n_minority,:],XY_train_orig[(n_minority+n_diff2):,:]))\n",
    "\n",
    "\n",
    "    # Build file paths.\n",
    "    file_X = f\"synthetic_nf/synthetic_X_seed{j}_ratio{best_ratio:.1f}.npy\"\n",
    "    file_y = f\"synthetic_nf/synthetic_y_seed{j}_ratio{best_ratio:.1f}.npy\"\n",
    "\n",
    "    # Load the synthetic data.\n",
    "    X_syn_full = np.load(file_X)\n",
    "    y_syn_full = np.load(file_y)\n",
    "\n",
    "    X_train_combined = np.vstack((\n",
    "        X_syn_full[:int(best_m*best_alpha), :],\n",
    "        X_syn_full[int(m/2):int(m/2+best_m*(1-best_alpha)), :]\n",
    "    ))\n",
    "    y_train_combined = np.vstack((\n",
    "        y_syn_full[:int(best_m*best_alpha), :],\n",
    "        y_syn_full[int(m/2):int(m/2+best_m*(1-best_alpha)), :]\n",
    "    ))\n",
    "\n",
    "    X_train_combined = np.vstack((X_train_combined, XY_reg[:, :-1]))\n",
    "    y_train_combined = np.vstack((y_train_combined, XY_reg[:, -1:]))\n",
    "\n",
    "    ce_one={}\n",
    "    # Train and evaluate on validation set\n",
    "    _, mse_val, mse_test, ypred_val, ypred_test = train_and_evaluate(\n",
    "        X_train_combined, y_train_combined, X_val, y_val, X_test, y_test)\n",
    "    ce_one['avg']=mse_test\n",
    "    \n",
    "    _, mse_val, mse_test, ypred_val, ypred_test = train_and_evaluate(\n",
    "    X_train_combined, y_train_combined, X_val, y_val, X_test[regions_test==1,], y_test[regions_test==1,])\n",
    "    ce_one['major']=mse_test\n",
    "        \n",
    "        \n",
    "    _, mse_val, mse_test, ypred_val, ypred_test = train_and_evaluate(\n",
    "    X_train_combined, y_train_combined, X_val, y_val, X_test[regions_test==0,], y_test[regions_test==0,])\n",
    "    ce_one['minor']=mse_test       \n",
    "    \n",
    "    ce_nf.append(ce_one)\n",
    "    \n",
    "    list_result.append(all_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0f3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_orig =[]\n",
    "for j in range(10):\n",
    "    set_seed(j)\n",
    "    \n",
    "    minority_idx = np.where(regions_total == 0)[0]\n",
    "    majority_idx = np.where(regions_total == 1)[0]\n",
    "\n",
    "    # Shuffle indices separately\n",
    "    np.random.shuffle(minority_idx)\n",
    "    np.random.shuffle(majority_idx)\n",
    "\n",
    "    # For minority:\n",
    "    min_train_idx = minority_idx[:n_minority]\n",
    "    min_val_idx   = minority_idx[n_minority : n_minority + n_val]\n",
    "    min_test_idx  = minority_idx[n_minority + n_val : n_minority + n_val + n_test]\n",
    "\n",
    "    # For majority:\n",
    "    maj_train_idx = majority_idx[:n_majority]\n",
    "    maj_val_idx   = majority_idx[n_majority : n_majority + n_val]\n",
    "    maj_test_idx  = majority_idx[n_majority + n_val : n_majority + n_val + n_test]\n",
    "\n",
    "    # Combine directly for each set\n",
    "    X_train_orig = np.vstack((X_total[min_train_idx], X_total[maj_train_idx]))\n",
    "    y_train_orig = np.vstack((y_total[min_train_idx], y_total[maj_train_idx]))\n",
    "    regions_train = np.concatenate((regions_total[min_train_idx], regions_total[maj_train_idx]))\n",
    "\n",
    "    X_val = np.vstack((X_total[min_val_idx], X_total[maj_val_idx]))\n",
    "    y_val = np.vstack((y_total[min_val_idx], y_total[maj_val_idx]))\n",
    "    regions_val = np.concatenate((regions_total[min_val_idx], regions_total[maj_val_idx]))\n",
    "\n",
    "    X_test = np.vstack((X_total[min_test_idx], X_total[maj_test_idx]))\n",
    "    y_test = np.vstack((y_total[min_test_idx], y_total[maj_test_idx]))\n",
    "    regions_test = np.concatenate((regions_total[min_test_idx], regions_total[maj_test_idx]))\n",
    "\n",
    "    # Combine training X and y for diffusion training\n",
    "    XY_train_orig = combine_XY(X_train_orig, y_train_orig)\n",
    "    n_train= XY_train_orig.shape[0]\n",
    "\n",
    "    ce_one={}\n",
    "    # Train and evaluate on validation set\n",
    "    _, mse_val, mse_test, ypred_val, ypred_test = train_and_evaluate(\n",
    "        X_train_orig, y_train_orig, X_val, y_val, X_test, y_test)\n",
    "    ce_one['avg']=mse_test\n",
    "    \n",
    "    _, mse_val, mse_test, ypred_val, ypred_test = train_and_evaluate(\n",
    "    X_train_orig, y_train_orig, X_val, y_val, X_test[regions_test==1,], y_test[regions_test==1,])\n",
    "    ce_one['major']=mse_test\n",
    "        \n",
    "        \n",
    "    _, mse_val, mse_test, ypred_val, ypred_test = train_and_evaluate(\n",
    "    X_train_orig, y_train_orig, X_val, y_val, X_test[regions_test==0,], y_test[regions_test==0,])\n",
    "    ce_one['minor']=mse_test       \n",
    "    \n",
    "    ce_orig.append(ce_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e194429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"result/res_nf.pkl\", \"wb\") as f:\n",
    "    pickle.dump([list_result,W_list,list_origin,ce_nf,ce_orig], f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eba47c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "with open(\"result/res_nf.pkl\", \"rb\") as f:\n",
    "    list_result,W_list,list_origin,ce_nf,ce_orig = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3c4c4",
   "metadata": {},
   "source": [
    "### Summary statistics for Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a411c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13917785453698983, 1.90899116771774, 1.024084511127365]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean([x['major'] for x in ce_orig]),np.mean([x['minor'] for x in ce_orig]),np.mean([x['avg'] for x in ce_orig])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2141bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.022445017522987557, 1.2482525909939948, 0.6262412031049229]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.std([x['major'] for x in ce_orig]),np.std([x['minor'] for x in ce_orig]),np.std([x['avg'] for x in ce_orig])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46058432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5269219934082979, 1.091464371011741, 0.8091931822100197]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean([x['major'] for x in ce_nf]),np.mean([x['minor'] for x in ce_nf]),np.mean([x['avg'] for x in ce_nf])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a030c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23046504428655476, 0.7772288612682747, 0.38353651238495734]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.std([x['major'] for x in ce_nf]),np.std([x['minor'] for x in ce_nf]),np.std([x['avg'] for x in ce_nf])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c008aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.3",
   "language": "python",
   "name": "python3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
