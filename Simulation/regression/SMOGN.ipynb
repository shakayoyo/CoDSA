{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedafe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/1/tianx/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "from toy_reg import *\n",
    "import ot\n",
    "import os\n",
    "import smogn\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d94e4a",
   "metadata": {},
   "source": [
    "## SMOGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e74af00",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_smogn=[]\n",
    "\n",
    "num_sim=10\n",
    "num_split= len(ratio_list )\n",
    "# Generate imbalanced data\n",
    "n_minority= 800\n",
    "n_majority = 3200\n",
    "n_val=400\n",
    "n_test=800\n",
    "\n",
    "list_origin=[]\n",
    "W_list=[[] for j in range(10)]\n",
    "X_total, y_total, regions_total = generate_imbalanced_data(\n",
    "        n_minority + n_val + n_test, \n",
    "        n_majority + n_val + n_test, \n",
    "        seed=0\n",
    "    )\n",
    "\n",
    "\n",
    "for j in range(num_sim):\n",
    "    set_seed(j)\n",
    "    \n",
    "    # Separate the minority and majority groups based on region labels.\n",
    "    # Here, we assume that region label 0 indicates minority and 1 indicates majority.\n",
    "    # Separate minority and majority indices\n",
    "    minority_idx = np.where(regions_total == 0)[0]\n",
    "    majority_idx = np.where(regions_total == 1)[0]\n",
    "\n",
    "    # Shuffle indices separately\n",
    "    np.random.shuffle(minority_idx)\n",
    "    np.random.shuffle(majority_idx)\n",
    "\n",
    "    # For minority:\n",
    "    min_train_idx = minority_idx[:n_minority]\n",
    "    min_val_idx   = minority_idx[n_minority : n_minority + n_val]\n",
    "    min_test_idx  = minority_idx[n_minority + n_val : n_minority + n_val + n_test]\n",
    "\n",
    "    # For majority:\n",
    "    maj_train_idx = majority_idx[:n_majority]\n",
    "    maj_val_idx   = majority_idx[n_majority : n_majority + n_val]\n",
    "    maj_test_idx  = majority_idx[n_majority + n_val : n_majority + n_val + n_test]\n",
    "\n",
    "    # Combine directly for each set\n",
    "    X_train_orig = np.vstack((X_total[min_train_idx], X_total[maj_train_idx]))\n",
    "    y_train_orig = np.vstack((y_total[min_train_idx], y_total[maj_train_idx]))\n",
    "    regions_train = np.concatenate((regions_total[min_train_idx], regions_total[maj_train_idx]))\n",
    "\n",
    "    X_val = np.vstack((X_total[min_val_idx], X_total[maj_val_idx]))\n",
    "    y_val = np.vstack((y_total[min_val_idx], y_total[maj_val_idx]))\n",
    "    regions_val = np.concatenate((regions_total[min_val_idx], regions_total[maj_val_idx]))\n",
    "\n",
    "    X_test = np.vstack((X_total[min_test_idx], X_total[maj_test_idx]))\n",
    "    y_test = np.vstack((y_total[min_test_idx], y_total[maj_test_idx]))\n",
    "    regions_test = np.concatenate((regions_total[min_test_idx], regions_total[maj_test_idx]))\n",
    "\n",
    "\n",
    "    XY_train_orig = combine_XY(X_train_orig, y_train_orig)\n",
    "    \n",
    "    n_train= XY_train_orig.shape[0]\n",
    "\n",
    "    origin_model, mse_val_origin, mse_test_origin, y_val_opred, y_test_opred = train_and_evaluate(\n",
    "    X_train_orig, y_train_orig, X_val, y_val, X_test, y_test)\n",
    "    list_origin.append([mse_val_origin, mse_test_origin])\n",
    "    print([mse_val_origin, mse_test_origin])\n",
    "\n",
    "    feature_cols = [f\"x{i}\" for i in range(X_train_orig.shape[1])]\n",
    "    df_train = pd.DataFrame(\n",
    "        np.hstack([X_train_orig, y_train_orig]),\n",
    "        columns=feature_cols + [\"target\"]\n",
    "    )\n",
    "\n",
    "    # 3. Define SMOGN hyperparameter grid\n",
    "    k_list        = [2]             # nearest neighbors\n",
    "    pert_list     = [0.02, 0.04, 0.06]     # perturbation\n",
    "    rel_thres_list= [0.2]     # relevance thresholds\n",
    "\n",
    "    best_mse_val = float(\"inf\")\n",
    "    best_params  = None\n",
    "\n",
    "    # 4. Grid search on validation set\n",
    "    for k in k_list:\n",
    "        for pert in pert_list:\n",
    "            for rel_thres in rel_thres_list:                \n",
    "                try:\n",
    "                    df_res = smogn.smoter(\n",
    "                        data=df_train,\n",
    "                        y=\"x0\",\n",
    "                        k=k,\n",
    "                        pert=pert,\n",
    "                        samp_method=\"balance\",\n",
    "                        drop_na_col=True,\n",
    "                        drop_na_row=True,\n",
    "                        replace=False,\n",
    "                        rel_method=\"auto\",\n",
    "                        rel_thres=rel_thres,\n",
    "                        rel_ctrl_pts_rg = [\n",
    "                            [0.5,  1, 0],  ## over-sample (\"minority\")\n",
    "                        ]\n",
    "                    )\n",
    "                except ValueError as e:\n",
    "                    # this often means rel_thres too low or phi all 1 → skip\n",
    "                    print(f\"Skipping k={k}, pert={pert}, rel_thres={rel_thres}: {e}\")\n",
    "                    continue\n",
    "                X_res = df_res[feature_cols].values\n",
    "                y_res = df_res[\"target\"].values.reshape(-1, 1)\n",
    "\n",
    "                _, mse_val, _, _, _ = train_and_evaluate(\n",
    "                    X_res, y_res,\n",
    "                    X_val, y_val,\n",
    "                    X_test, y_test\n",
    "                )\n",
    "                print(f\"SMOGN k={k}, pert={pert}, rel_thres={rel_thres} → val MSE: {mse_val:.4f}\")\n",
    "                if mse_val < best_mse_val:\n",
    "                    best_mse_val = mse_val\n",
    "                    best_params   = (k, pert, rel_thres)\n",
    "\n",
    "    print(f\"Best SMOGN params: k={best_params[0]}, pert={best_params[1]}, rel_thres={best_params[2]} → val MSE: {best_mse_val:.4f}\")\n",
    "\n",
    "    if  best_params  == None:\n",
    "        continue\n",
    "    # 5. Resample with best hyperparameters and evaluate on test set\n",
    "    k_best, pert_best, rel_thres_best = best_params\n",
    "    df_res_best = smogn.smoter(\n",
    "        data=df_train,\n",
    "        y=\"x0\",\n",
    "        k=k_best,\n",
    "        pert=pert_best,\n",
    "        samp_method=\"balance\",\n",
    "        drop_na_col=True,\n",
    "        drop_na_row=True,\n",
    "        replace=False,\n",
    "        rel_method=\"auto\",\n",
    "        rel_thres=rel_thres_best,\n",
    "        rel_ctrl_pts_rg = [\n",
    "                            [0.5,  1, 0],  ## over-sample (\"minority\")\n",
    "                        ]\n",
    "    )\n",
    "    X_smogn = df_res_best[feature_cols].values\n",
    "    y_smogn = df_res_best[\"target\"].values.reshape(-1, 1)\n",
    "\n",
    "    _, _, mse_test_smogn, _, _ = train_and_evaluate(\n",
    "        X_smogn, y_smogn,\n",
    "        X_val, y_val,\n",
    "        X_test, y_test\n",
    "    )\n",
    "    print(f\"{j}:SMOGN final → val MSE: {mse_test_smogn:.4f}\")\n",
    "\n",
    "    # 6. Record the result\n",
    "    ce_smogn.append({\n",
    "        \"avg\": mse_val_smogn,\n",
    "        \"major\": train_and_evaluate(X_smogn, y_smogn, X_val, y_val, X_test[regions_test==1], y_test[regions_test==1])[2],\n",
    "        \"minor\": train_and_evaluate(X_smogn, y_smogn, X_val, y_val, X_test[regions_test==0], y_test[regions_test==0])[2],\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7af10a",
   "metadata": {},
   "source": [
    "### Summary statistics for Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "279e2218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.23120323608828502, 1.5534241096310915, 0.8413967777816665]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean([x['major'] for x in ce_smogn]),np.mean([x['minor'] for x in ce_smogn]),np.mean([x['avg'] for x in ce_smogn])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d5355d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.10755476039767976, 1.0576341101608233, 0.4715261182790246]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.std([x['major'] for x in ce_smogn]),np.std([x['minor'] for x in ce_smogn]),np.std([x['avg'] for x in ce_smogn])] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952897a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.8.3",
   "language": "python",
   "name": "python3.8.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
